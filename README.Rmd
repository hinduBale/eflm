---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r setup, include = FALSE}
library(eflm)
library(ggplot2)
library(patchwork)

library(readxl)
library(dplyr)
library(tidyr)
library(wesanderson)
library(forcats)

knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# Efficient Fitting of Linear and Generalized Linear Models <img src="https://pacha.dev/eflm/hexicon.svg" width=150 align="right" alt="sticker"/>

<!-- badges: start -->
[![Project Status: Active â€“ The project has reached a stable, usable
state and is being actively
developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)
[![Lifecycle: stable](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg)](https://www.tidyverse.org/lifecycle/#stable)
[![CRAN
status](https://www.r-pkg.org/badges/version/gravity)](https://cran.r-project.org/package=eflm)
[![codecov](https://codecov.io/gh/pachamaltese/eflm/branch/main/graph/badge.svg?token=XI59cmGd15)](https://codecov.io/gh/pachamaltese/eflm)
[![R-CMD-check](https://github.com/pachamaltese/yotover-testing/workflows/R-CMD-check/badge.svg)](https://github.com/pachamaltese/yotover-testing/actions)
<!-- badges: end -->

## Description

Efficient Fitting of Linear and Generalized Linear Models by using
just base R. As an alternative to `lm()` and `glm()`, this package provides
`elm()` and `eglm()`, with a significant speedup when the number of 
observations is larger than the number of parameters to estimate. The speed
gains are obtained by reducing the NxP model matrix to a PxP matrix, and the 
best computational performance is obtained when R is linked against OpenBLAS,
Intel MKL or other optimized BLAS library. This implementation aims at being
compatible with 'broom' and 'sandwich' packages for summary statistics and
clustering by providing S3 methods.

## Details

This package takes ideas from glm2, speedglm, fastglm, and fixest packages, but
the implementations here shall keep the functions and outputs as closely as
possible to the stats package, therefore making the functions provided here
compatible with packages such as sandwich for robust estimation, even if that
means to attenuate the speed gains.

The greatest strength of this package is testing. With more than 750 
(and counting) tests, we try to do exactly the same as lm/glm, even in edge 
cases, but faster.

The ultimate aim of the project is to produce a package that:

* Does exactly the same as lm and glm in less time
* Is equally numerically stable as lm and glm
* Depends only on base R, with no Rcpp or other calls
* Uses R's internal C code such as the `Cdqrls` function that the stats package uses for model fitting
* Can be used in Shiny dashboard and contexts where you need fast model fitting
* Is useful for memory consuming models
* Allows model fitting with limited hardware

## Minimal working example

### Stats (base) package
```{r message=FALSE, warning=FALSE}
formula <- "mpg ~ I(wt^2)"
summary(glm(formula, data = mtcars))
```

### Eflm package

```{r message=FALSE, warning=FALSE}
library(eflm)
summary(eglm(formula, data = mtcars))
```

## Installation

You can install the released version of eflm from CRAN with:
```r
install.packages("eflm")
```

And the development version with:
``` r
remotes::install_github("pachamaltese/eflm")
```

## Benchmarks

Let's fit two computationally demanding models from [Yotov, et al. (2016)](https://pacha.dev/yotover/).

The dataset for the benchmark was also taken from Yotov, et al. and consists in a 28,152 x 8 data frame with 6 numeric and 2 categorical columns:

```{r}
trade_data_yotov
```

The variables are:

* year: time of export/import flow
* trade: bilateral trade
* log_dist: log of distance
* cntg: contiguity
* lang: common language
* clny: colonial relation
* exp_year/imp_year: exporter/importer time fixed effects

### OLS estimation controlling for multilateral resistance terms with fixed effects

*This test was conducted on a 2 dedicated CPUs and 4GB of RAM DigitalOcean droplet.*

The general equation for this model is: 

\begin{align}
\log X_{ij,t} =& \:\pi_{i,t} + \chi_{i,t} + \beta_1 \log(DIST)_{i,j} + \beta_2 CNTG_{i,j} + \beta_3 LANG_{i,j} +\\
\text{ }& \:\beta_4 CLNY_{i,j} + \varepsilon_{ij,t}
\end{align}

By running regressions with cumulative subset of the data for 1986, ..., 2006 (e.g. regress for 1986, then 1986 and 1990, ..., then 1986 to 2006), we obtain the next fitting times depending on the design matrix dimensions:

```{r echo=FALSE}
benchmark <- do.call("rbind", trade_data_yotov_benchmark$ols)
benchmark$dimensions <- benchmark$mm_rows * benchmark$mm_cols
benchmark$expression2 <- ifelse(grepl("elm", benchmark$expression), "ELM", "LM")

g1 <- ggplot(benchmark, aes(x = dimensions, y = median, color = expression2)) +
  geom_point() +
  geom_line() +
  bench::scale_y_bench_time(NULL) +
  scale_x_log10(
    breaks = scales::trans_breaks("log10", function(x) 10^x),
    labels = scales::trans_format("log10", scales::math_format(10^.x))
  ) +
  # coord_fixed() +
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        legend.title = element_blank()) +
  labs(title = "Observations vs Fitting Time",
       x = "Number of Observations",
       y = "Fitting Time") +
  scale_color_manual(values = wes_palette("Zissou1")[c(1,4)])

g2 <- ggplot(benchmark, aes(x = dimensions, y = mem_alloc, color = expression2)) +
  geom_point() +
  geom_line() +
  bench::scale_y_bench_bytes(NULL) +
  scale_x_log10(
    breaks = scales::trans_breaks("log10", function(x) 10^x),
    labels = scales::trans_format("log10", scales::math_format(10^.x))
  ) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        legend.title = element_blank()) +
  labs(title = "Observations vs Memory Allocation",
       x = "Number of Observations",
       y = "Memory Allocation") +
  scale_color_manual(values = wes_palette("Zissou1")[c(1,4)])

g1 / g2
```

The general equation for this model is: 

### PPML estimation controlling for multilateral resistance terms with fixed effects:

*This test was conducted on a 2 dedicated CPUs and 4GB of RAM DigitalOcean droplet.*

\begin{align}
X_{ij,t} =& \:\exp\left[\pi_{i,t} + \chi_{i,t} + \beta_1 \log(DIST)_{i,j} + \beta_2 CNTG_{i,j} +\right.\\
\text{ }& \:\left.\beta_3 LANG_{i,j} + \beta_4 CLNY_{i,j}\right] \times \varepsilon_{ij,t}
\end{align}

By running regressions with cumulative subset of the data for 1986, ..., 2006 (e.g. regress for 1986, then 1986 and 1990, ..., then 1986 to 2006), we obtain the next fitting times depending on the design matrix dimensions:

```{r echo=FALSE}
benchmark <- do.call("rbind", trade_data_yotov_benchmark$ppml)
benchmark$dimensions <- benchmark$mm_rows * benchmark$mm_cols
benchmark$expression2 <- ifelse(grepl("eglm", benchmark$expression), "EGLM", "GLM")

g1 <- ggplot(benchmark, aes(x = dimensions, y = median, color = expression2)) +
  geom_point() +
  geom_line() +
  bench::scale_y_bench_time(NULL) +
  scale_x_log10(
    breaks = scales::trans_breaks("log10", function(x) 10^x),
    labels = scales::trans_format("log10", scales::math_format(10^.x))
  ) +
  # coord_fixed() +
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        legend.title = element_blank()) +
  labs(title = "Observations vs Fitting Time",
       x = "Number of Observations",
       y = "Fitting Time") +
  scale_color_manual(values = wes_palette("Zissou1")[c(1,4)]) 

g2 <- ggplot(benchmark, aes(x = dimensions, y = mem_alloc, color = expression2)) +
  geom_point() +
  geom_line() +
  bench::scale_y_bench_bytes(NULL) +
  scale_x_log10(
    breaks = scales::trans_breaks("log10", function(x) 10^x),
    labels = scales::trans_format("log10", scales::math_format(10^.x))
  ) +
  theme_minimal() +
  theme(panel.grid.minor = element_blank(),
        legend.title = element_blank()) +
  labs(title = "Observations vs Memory Allocation",
       x = "Number of Observations",
       y = "Memory Allocation") +
  scale_color_manual(values = wes_palette("Zissou1")[c(1,4)])

g1 / g2
```

### Performance on scaled hardware

*This test was conducted on different dedicated CPUs DigitalOcean droplets.*

We can repeatedly run the Quasi-Poisson regression with the full dataset and compare the results on different hardware. 

The results which are very consistent across different hardware (this is surprising, more CPUs don't reduce the median fitting time). The next plot summarises 4,000 repetitions of the tests, with 500 repetions for both `glm()` and `eglm()` on different hardware:

```{r echo=FALSE, fig.height=7, fig.width=10, dpi = 150}
tidy_benchmarks <- read_excel("dev/tidy benchmarks.xlsx")

tidy_benchmarks %>%
  select(-expr, -neval) %>%
  gather(Statistic, `Time (Seconds)`, -Hardware, -Function) %>%
  mutate(
    Statistic = as_factor(Statistic),
    Statistic = fct_relevel(Statistic, "Min", "LQ", "Median",
                            "Mean", "UQ", "Max")
  ) %>%
  ggplot() +
  geom_col(aes(x = Function, y = `Time (Seconds)`, fill = Hardware), position = "dodge2") +
  facet_wrap(~Statistic) +
  scale_fill_manual(values = wes_palette("Zissou1")) +
  labs(title = "Benchmarking Time on DigitalOcean Droplets") +
  theme_minimal(base_size = 14)
```

## Progress list

### Sandwich compatibility

- [x] estfun
- [x] bread
- [x] vcovCL
- [x] meatCL
- [x] vcovCL
- [x] vcovBS
- [ ] vcovHC
- [ ] meatHC
- [ ] vcovPC
- [ ] meatPC
- [ ] vcovPL
- [ ] meatPL

### Broom compatibility

- [x] augment
- [x] tidy

### CAR 

RESIDUALPLOTS
cooks.distance?
hatvalues?
influenceIndexPlot
influencePlot
QQpLOT
compareCoefs

ver fit4 <- update(fit, subset=!(rownames(datos2) %in% c("NY","SD","TX","NV","CT"))  )
