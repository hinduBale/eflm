---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# Efficient Fitting of Linear and Generalized Linear Models <img src="https://pacha.dev/eflm/hexicon.svg" width=150 align="right" alt="sticker"/>

<!-- badges: start -->
[![Project Status: Active â€“ The project has reached a stable, usable
state and is being actively
developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active)
[![Lifecycle: stable](https://img.shields.io/badge/lifecycle-stable-brightgreen.svg)](https://www.tidyverse.org/lifecycle/#stable)
[![CRAN
status](https://www.r-pkg.org/badges/version/gravity)](https://cran.r-project.org/package=eflm)
[![codecov](https://codecov.io/gh/pachamaltese/eflm/branch/main/graph/badge.svg?token=XI59cmGd15)](https://codecov.io/gh/pachamaltese/eflm)
[![R-CMD-check](https://github.com/pachamaltese/yotover-testing/workflows/R-CMD-check/badge.svg)](https://github.com/pachamaltese/yotover-testing/actions)
<!-- badges: end -->

## Description

Efficient Fitting of Linear and Generalized Linear Models by using
just base R. As an alternative to `lm()` and `glm()`, this package provides
`elm()` and `eglm()`, with a significant speedup when the number of 
observations is larger than the number of parameters to estimate. The speed
gains are obtained by reducing the NxP model matrix to a PxP matrix, and the 
best computational performance is obtained when R is linked against OpenBLAS,
Intel MKL or other optimized BLAS library. This implementation aims at being
compatible with 'broom' and 'sandwich' packages for summary statistics and
clustering by providing S3 methods.

## Details

This package takes ideas from glm2, speedglm, fastglm, and fixest packages, but
the implementations here shall keep the functions and outputs as closely as
possible to the stats package, therefore making the functions provided here
compatible with packages such as sandwich for robust estimation, even if that
means to attenuate the speed gains.

The greatest strength of this package is testing. With more than 750 
(and counting) tests, we try to do exactly the same as lm/glm, even in edge 
cases, but faster.

The ultimate aim of the project is to produce a package that:

* Does exactly the same as lm and glm in less time
* Is equally numerically stable as lm and glm
* Depends only on base R, with no Rcpp or other calls
* Uses R's internal C code such as the `Cdqrls` function that the stats package uses for model fitting
* Can be used in Shiny dashboard and contexts where you need fast model fitting
* Is useful for memory consuming models
* Allows model fitting with limited hardware

## Minimal working example

### Stats (base) package
```{r message=FALSE, warning=FALSE}
formula <- "mpg ~ I(wt^2)"
summary(glm(formula, data = mtcars))
```

### Eflm package

```{r message=FALSE, warning=FALSE}
library(eflm)
summary(eglm(formula, data = mtcars))
```

## Installation

You can install the released version of eflm from CRAN with:
```r
install.packages("eflm")
```

And the development version with:
``` r
remotes::install_github("pachamaltese/eflm")
```

## Benchmarks

*PENDING*: Add more tests in vignette

I fitted a computationally complex model from [Yotov, et. al. (2016)](https://pacha.dev/yotover/partial-equilibrium-trade-policy-analysis-with-structural-gravity.html#ppml-estimation-controlling-for-multilateral-resistance-terms-with-fixed-effects). The benchmark was the model:
```
trade ~ log_dist + cntg + lang + clny + exp_year + imp_year
```

The variables are:

* trade: bilateral trade
* log_dist: log of distance
* cntg: contiguity
* lang: common language
* clny: colonial relation
* exp_year/imp_year: exporter/importer time fixed effects

The data for this model consists in a 28,152 x 7 data frame with 5 numeric and 2 categorical columns. This results in a 28,152 x 23 design matrix:

```{r echo=FALSE, message=FALSE, warning=FALSE}
library(yotover)
ch1_application1_2 <- readRDS("~/github/eflm/dev/ch1_application1_2.rds")
ch1_application1_2
```

The benchmark was conducted by using the microbenchmark package and running the code in different Digital Ocean droplets:

```{r, eval = FALSE}
benchmark_times <- microbenchmark(
  glm(trade ~ log_dist + cntg + lang + clny + exp_year + imp_year,
      family = quasipoisson(link = "log"),
      data = ch1_application1_2,
      y = FALSE,
      model = FALSE
  ),
  eglm(trade ~ log_dist + cntg + lang + clny + exp_year + imp_year,
       family = quasipoisson(link = "log"),
       data = ch1_application1_2,
       y = FALSE,
       model = FALSE
  ),
  times = 500L
)
```

Here are the tests results which are very consistent across different hardware (this is surprising, more CPUs don't reduce the median time). Also notice that this plot summarises 2,000 repetitions of the tests:
```{r echo=FALSE, fig.height=7, fig.width=10, dpi = 150}
library(ggplot2)
library(readxl)
library(dplyr)
library(tidyr)
library(wesanderson)
library(forcats)

tidy_benchmarks <- read_excel("dev/tidy benchmarks.xlsx")

tidy_benchmarks %>%
  select(-expr, -neval) %>%
  gather(Statistic, `Time (Seconds)`, -Hardware, -Function) %>%
  mutate(
    Statistic = as_factor(Statistic),
    Statistic = fct_relevel(Statistic, "Min", "LQ", "Median",
                            "Mean", "UQ", "Max")
  ) %>%
  ggplot() +
  geom_col(aes(x = Function, y = `Time (Seconds)`, fill = Hardware), position = "dodge2") +
  facet_wrap(~Statistic) +
  scale_fill_manual(values = wes_palette("Zissou1")) +
  labs(title = "Benchmarking Time on DigitalOcean Droplets") +
  theme_minimal(base_size = 14)
```

## Progress list

### Sandwich compatibility

- [x] estfun
- [x] bread
- [x] vcovCL
- [x] meatCL
- [x] vcovCL
- [x] vcovBS
- [ ] vcovHC
- [ ] meatHC
- [ ] vcovPC
- [ ] meatPC
- [ ] vcovPL
- [ ] meatPL

### Broom compatibility

- [x] augment
- [x] tidy
